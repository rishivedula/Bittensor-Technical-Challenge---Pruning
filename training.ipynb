{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    If GPU is available then run on GPU else run on CPU\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    . torchvision.datasets.MNIST downloads PIL images \n",
    "    . Hence we need to define transform object to convert downloaded MNIST PIL images to Tensors\n",
    "    . Batch size of 32 was used. \n",
    "    \n",
    "    Reference --> https://pytorch.org/vision/stable/datasets.html#mnist\n",
    "    \n",
    "\"\"\"\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Creating Data Loader object for creating the Training set by using the field train=True  \n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\"./\",train=True,download=True ,transform=transform)\n",
    "train_data_loaded = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "# Creating Data Loader object for creating the Test set by using the field train=False \n",
    "test_data = torchvision.datasets.MNIST(\"./\",train=False,download=True,transform=transform)\n",
    "test_data_loaded = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating the Model\n",
    "\n",
    "input (inp) ==> 28*28 neurons\n",
    "layer1      ==> 1000 neurons\n",
    "layer2      ==> 1000 neurons\n",
    "layer3      ==> 500 neurons\n",
    "layer4      ==> 200 neurons\n",
    "output(out) ==> 10 neurons  Since number of classes are 10\n",
    "\n",
    "* Softmax layer is not being used as the final layer becuase Crossentropy loss\n",
    "  is being used which takes the input as logits and calculates softmax before computing loss. \n",
    "  \n",
    "  Reference --> https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    # Intialising the number of neurons in the layers\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.inp = nn.Linear(28*28,1000)\n",
    "        self.layer1 = nn.Linear(1000,1000)\n",
    "        self.layer2 = nn.Linear(1000,1000)\n",
    "        self.layer3 = nn.Linear(1000,500)\n",
    "        self.layer4 = nn.Linear(500,200)\n",
    "        self.out = nn.Linear(200,10)\n",
    "    \n",
    "    # Defining the forward pass of the model\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.inp(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim = 1)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (inp): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (layer1): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (layer2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (layer3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (layer4): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (out): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.to(torch.device(\"cuda:0\"))                   # Loading the model to GPU is avalible else CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   1) Cross Entropy loss is being used because this is a classification model.\n",
    "   2) Softmax of the ouput logits is first calculated and then classified to calculate the loss \n",
    "   3) Adam Optimizer is used. \n",
    "\"\"\"\n",
    "criterion = torch.nn.CrossEntropyLoss()            # initializing Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters())   # initializing Adam Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.076\n",
      "[1,   400] loss: 0.032\n",
      "[1,   600] loss: 0.026\n",
      "[1,   800] loss: 0.024\n",
      "[1,  1000] loss: 0.021\n",
      "[1,  1200] loss: 0.019\n",
      "[1,  1400] loss: 0.018\n",
      "[1,  1600] loss: 0.017\n",
      "[1,  1800] loss: 0.016\n",
      "[2,   200] loss: 0.012\n",
      "[2,   400] loss: 0.012\n",
      "[2,   600] loss: 0.014\n",
      "[2,   800] loss: 0.016\n",
      "[2,  1000] loss: 0.013\n",
      "[2,  1200] loss: 0.011\n",
      "[2,  1400] loss: 0.012\n",
      "[2,  1600] loss: 0.011\n",
      "[2,  1800] loss: 0.013\n",
      "[3,   200] loss: 0.009\n",
      "[3,   400] loss: 0.010\n",
      "[3,   600] loss: 0.010\n",
      "[3,   800] loss: 0.008\n",
      "[3,  1000] loss: 0.010\n",
      "[3,  1200] loss: 0.010\n",
      "[3,  1400] loss: 0.009\n",
      "[3,  1600] loss: 0.010\n",
      "[3,  1800] loss: 0.008\n",
      "[4,   200] loss: 0.008\n",
      "[4,   400] loss: 0.008\n",
      "[4,   600] loss: 0.007\n",
      "[4,   800] loss: 0.007\n",
      "[4,  1000] loss: 0.008\n",
      "[4,  1200] loss: 0.006\n",
      "[4,  1400] loss: 0.007\n",
      "[4,  1600] loss: 0.007\n",
      "[4,  1800] loss: 0.007\n",
      "[5,   200] loss: 0.007\n",
      "[5,   400] loss: 0.007\n",
      "[5,   600] loss: 0.006\n",
      "[5,   800] loss: 0.006\n",
      "[5,  1000] loss: 0.006\n",
      "[5,  1200] loss: 0.007\n",
      "[5,  1400] loss: 0.006\n",
      "[5,  1600] loss: 0.005\n",
      "[5,  1800] loss: 0.006\n",
      "[6,   200] loss: 0.004\n",
      "[6,   400] loss: 0.004\n",
      "[6,   600] loss: 0.006\n",
      "[6,   800] loss: 0.006\n",
      "[6,  1000] loss: 0.005\n",
      "[6,  1200] loss: 0.006\n",
      "[6,  1400] loss: 0.005\n",
      "[6,  1600] loss: 0.005\n",
      "[6,  1800] loss: 0.006\n",
      "[7,   200] loss: 0.003\n",
      "[7,   400] loss: 0.005\n",
      "[7,   600] loss: 0.005\n",
      "[7,   800] loss: 0.006\n",
      "[7,  1000] loss: 0.004\n",
      "[7,  1200] loss: 0.004\n",
      "[7,  1400] loss: 0.004\n",
      "[7,  1600] loss: 0.006\n",
      "[7,  1800] loss: 0.005\n",
      "[8,   200] loss: 0.003\n",
      "[8,   400] loss: 0.003\n",
      "[8,   600] loss: 0.003\n",
      "[8,   800] loss: 0.005\n",
      "[8,  1000] loss: 0.005\n",
      "[8,  1200] loss: 0.005\n",
      "[8,  1400] loss: 0.004\n",
      "[8,  1600] loss: 0.004\n",
      "[8,  1800] loss: 0.004\n",
      "[9,   200] loss: 0.004\n",
      "[9,   400] loss: 0.004\n",
      "[9,   600] loss: 0.004\n",
      "[9,   800] loss: 0.004\n",
      "[9,  1000] loss: 0.004\n",
      "[9,  1200] loss: 0.003\n",
      "[9,  1400] loss: 0.004\n",
      "[9,  1600] loss: 0.004\n",
      "[9,  1800] loss: 0.005\n",
      "[10,   200] loss: 0.003\n",
      "[10,   400] loss: 0.003\n",
      "[10,   600] loss: 0.004\n",
      "[10,   800] loss: 0.004\n",
      "[10,  1000] loss: 0.003\n",
      "[10,  1200] loss: 0.005\n",
      "[10,  1400] loss: 0.004\n",
      "[10,  1600] loss: 0.003\n",
      "[10,  1800] loss: 0.004\n",
      "[11,   200] loss: 0.002\n",
      "[11,   400] loss: 0.003\n",
      "[11,   600] loss: 0.002\n",
      "[11,   800] loss: 0.003\n",
      "[11,  1000] loss: 0.006\n",
      "[11,  1200] loss: 0.003\n",
      "[11,  1400] loss: 0.003\n",
      "[11,  1600] loss: 0.003\n",
      "[11,  1800] loss: 0.003\n",
      "[12,   200] loss: 0.001\n",
      "[12,   400] loss: 0.003\n",
      "[12,   600] loss: 0.002\n",
      "[12,   800] loss: 0.002\n",
      "[12,  1000] loss: 0.003\n",
      "[12,  1200] loss: 0.003\n",
      "[12,  1400] loss: 0.003\n",
      "[12,  1600] loss: 0.003\n",
      "[12,  1800] loss: 0.008\n",
      "[13,   200] loss: 0.005\n",
      "[13,   400] loss: 0.003\n",
      "[13,   600] loss: 0.002\n",
      "[13,   800] loss: 0.002\n",
      "[13,  1000] loss: 0.003\n",
      "[13,  1200] loss: 0.002\n",
      "[13,  1400] loss: 0.002\n",
      "[13,  1600] loss: 0.005\n",
      "[13,  1800] loss: 0.003\n",
      "[14,   200] loss: 0.002\n",
      "[14,   400] loss: 0.004\n",
      "[14,   600] loss: 0.004\n",
      "[14,   800] loss: 0.002\n",
      "[14,  1000] loss: 0.003\n",
      "[14,  1200] loss: 0.002\n",
      "[14,  1400] loss: 0.003\n",
      "[14,  1600] loss: 0.002\n",
      "[14,  1800] loss: 0.003\n",
      "[15,   200] loss: 0.002\n",
      "[15,   400] loss: 0.002\n",
      "[15,   600] loss: 0.001\n",
      "[15,   800] loss: 0.001\n",
      "[15,  1000] loss: 0.004\n",
      "[15,  1200] loss: 0.003\n",
      "[15,  1400] loss: 0.003\n",
      "[15,  1600] loss: 0.004\n",
      "[15,  1800] loss: 0.003\n",
      "[16,   200] loss: 0.002\n",
      "[16,   400] loss: 0.002\n",
      "[16,   600] loss: 0.003\n",
      "[16,   800] loss: 0.002\n",
      "[16,  1000] loss: 0.002\n",
      "[16,  1200] loss: 0.002\n",
      "[16,  1400] loss: 0.002\n",
      "[16,  1600] loss: 0.002\n",
      "[16,  1800] loss: 0.002\n",
      "[17,   200] loss: 0.002\n",
      "[17,   400] loss: 0.003\n",
      "[17,   600] loss: 0.002\n",
      "[17,   800] loss: 0.002\n",
      "[17,  1000] loss: 0.003\n",
      "[17,  1200] loss: 0.002\n",
      "[17,  1400] loss: 0.005\n",
      "[17,  1600] loss: 0.004\n",
      "[17,  1800] loss: 0.003\n",
      "[18,   200] loss: 0.002\n",
      "[18,   400] loss: 0.002\n",
      "[18,   600] loss: 0.001\n",
      "[18,   800] loss: 0.002\n",
      "[18,  1000] loss: 0.003\n",
      "[18,  1200] loss: 0.004\n",
      "[18,  1400] loss: 0.003\n",
      "[18,  1600] loss: 0.003\n",
      "[18,  1800] loss: 0.004\n",
      "[19,   200] loss: 0.001\n",
      "[19,   400] loss: 0.000\n",
      "[19,   600] loss: 0.004\n",
      "[19,   800] loss: 0.003\n",
      "[19,  1000] loss: 0.002\n",
      "[19,  1200] loss: 0.002\n",
      "[19,  1400] loss: 0.002\n",
      "[19,  1600] loss: 0.003\n",
      "[19,  1800] loss: 0.001\n",
      "[20,   200] loss: 0.002\n",
      "[20,   400] loss: 0.002\n",
      "[20,   600] loss: 0.002\n",
      "[20,   800] loss: 0.002\n",
      "[20,  1000] loss: 0.002\n",
      "[20,  1200] loss: 0.002\n",
      "[20,  1400] loss: 0.001\n",
      "[20,  1600] loss: 0.003\n",
      "[20,  1800] loss: 0.002\n",
      "[21,   200] loss: 0.002\n",
      "[21,   400] loss: 0.002\n",
      "[21,   600] loss: 0.004\n",
      "[21,   800] loss: 0.002\n",
      "[21,  1000] loss: 0.002\n",
      "[21,  1200] loss: 0.003\n",
      "[21,  1400] loss: 0.003\n",
      "[21,  1600] loss: 0.002\n",
      "[21,  1800] loss: 0.003\n",
      "[22,   200] loss: 0.001\n",
      "[22,   400] loss: 0.003\n",
      "[22,   600] loss: 0.001\n",
      "[22,   800] loss: 0.001\n",
      "[22,  1000] loss: 0.002\n",
      "[22,  1200] loss: 0.001\n",
      "[22,  1400] loss: 0.002\n",
      "[22,  1600] loss: 0.001\n",
      "[22,  1800] loss: 0.001\n",
      "[23,   200] loss: 0.001\n",
      "[23,   400] loss: 0.003\n",
      "[23,   600] loss: 0.001\n",
      "[23,   800] loss: 0.004\n",
      "[23,  1000] loss: 0.002\n",
      "[23,  1200] loss: 0.001\n",
      "[23,  1400] loss: 0.002\n",
      "[23,  1600] loss: 0.002\n",
      "[23,  1800] loss: 0.001\n",
      "[24,   200] loss: 0.002\n",
      "[24,   400] loss: 0.001\n",
      "[24,   600] loss: 0.002\n",
      "[24,   800] loss: 0.001\n",
      "[24,  1000] loss: 0.003\n",
      "[24,  1200] loss: 0.003\n",
      "[24,  1400] loss: 0.004\n",
      "[24,  1600] loss: 0.002\n",
      "[24,  1800] loss: 0.003\n",
      "[25,   200] loss: 0.000\n",
      "[25,   400] loss: 0.000\n",
      "[25,   600] loss: 0.001\n",
      "[25,   800] loss: 0.001\n",
      "[25,  1000] loss: 0.002\n",
      "[25,  1200] loss: 0.003\n",
      "[25,  1400] loss: 0.002\n",
      "[25,  1600] loss: 0.002\n",
      "[25,  1800] loss: 0.002\n",
      "[26,   200] loss: 0.001\n",
      "[26,   400] loss: 0.003\n",
      "[26,   600] loss: 0.003\n",
      "[26,   800] loss: 0.001\n",
      "[26,  1000] loss: 0.001\n",
      "[26,  1200] loss: 0.000\n",
      "[26,  1400] loss: 0.001\n",
      "[26,  1600] loss: 0.001\n",
      "[26,  1800] loss: 0.004\n",
      "[27,   200] loss: 0.001\n",
      "[27,   400] loss: 0.003\n",
      "[27,   600] loss: 0.002\n",
      "[27,   800] loss: 0.002\n",
      "[27,  1000] loss: 0.002\n",
      "[27,  1200] loss: 0.002\n",
      "[27,  1400] loss: 0.004\n",
      "[27,  1600] loss: 0.002\n",
      "[27,  1800] loss: 0.001\n",
      "[28,   200] loss: 0.001\n",
      "[28,   400] loss: 0.001\n",
      "[28,   600] loss: 0.001\n",
      "[28,   800] loss: 0.002\n",
      "[28,  1000] loss: 0.001\n",
      "[28,  1200] loss: 0.000\n",
      "[28,  1400] loss: 0.004\n",
      "[28,  1600] loss: 0.004\n",
      "[28,  1800] loss: 0.002\n",
      "[29,   200] loss: 0.000\n",
      "[29,   400] loss: 0.001\n",
      "[29,   600] loss: 0.002\n",
      "[29,   800] loss: 0.001\n",
      "[29,  1000] loss: 0.002\n",
      "[29,  1200] loss: 0.001\n",
      "[29,  1400] loss: 0.002\n",
      "[29,  1600] loss: 0.003\n",
      "[29,  1800] loss: 0.003\n",
      "[30,   200] loss: 0.001\n",
      "[30,   400] loss: 0.002\n",
      "[30,   600] loss: 0.003\n",
      "[30,   800] loss: 0.001\n",
      "[30,  1000] loss: 0.002\n",
      "[30,  1200] loss: 0.002\n",
      "[30,  1400] loss: 0.001\n",
      "[30,  1600] loss: 0.001\n",
      "[30,  1800] loss: 0.002\n",
      "[31,   200] loss: 0.003\n",
      "[31,   400] loss: 0.001\n",
      "[31,   600] loss: 0.000\n",
      "[31,   800] loss: 0.002\n",
      "[31,  1000] loss: 0.002\n",
      "[31,  1200] loss: 0.002\n",
      "[31,  1400] loss: 0.003\n",
      "[31,  1600] loss: 0.003\n",
      "[31,  1800] loss: 0.004\n",
      "[32,   200] loss: 0.001\n",
      "[32,   400] loss: 0.003\n",
      "[32,   600] loss: 0.004\n",
      "[32,   800] loss: 0.005\n",
      "[32,  1000] loss: 0.003\n",
      "[32,  1200] loss: 0.001\n",
      "[32,  1400] loss: 0.007\n",
      "[32,  1600] loss: 0.002\n",
      "[32,  1800] loss: 0.001\n",
      "[33,   200] loss: 0.001\n",
      "[33,   400] loss: 0.001\n",
      "[33,   600] loss: 0.000\n",
      "[33,   800] loss: 0.001\n",
      "[33,  1000] loss: 0.004\n",
      "[33,  1200] loss: 0.001\n",
      "[33,  1400] loss: 0.001\n",
      "[33,  1600] loss: 0.003\n",
      "[33,  1800] loss: 0.002\n",
      "[34,   200] loss: 0.002\n",
      "[34,   400] loss: 0.001\n",
      "[34,   600] loss: 0.006\n",
      "[34,   800] loss: 0.002\n",
      "[34,  1000] loss: 0.001\n",
      "[34,  1200] loss: 0.001\n",
      "[34,  1400] loss: 0.002\n",
      "[34,  1600] loss: 0.001\n",
      "[34,  1800] loss: 0.002\n",
      "[35,   200] loss: 0.001\n",
      "[35,   400] loss: 0.004\n",
      "[35,   600] loss: 0.002\n",
      "[35,   800] loss: 0.001\n",
      "[35,  1000] loss: 0.003\n",
      "[35,  1200] loss: 0.001\n",
      "[35,  1400] loss: 0.002\n",
      "[35,  1600] loss: 0.002\n",
      "[35,  1800] loss: 0.001\n",
      "[36,   200] loss: 0.000\n",
      "[36,   400] loss: 0.000\n",
      "[36,   600] loss: 0.002\n",
      "[36,   800] loss: 0.002\n",
      "[36,  1000] loss: 0.001\n",
      "[36,  1200] loss: 0.001\n",
      "[36,  1400] loss: 0.000\n",
      "[36,  1600] loss: 0.003\n",
      "[36,  1800] loss: 0.004\n",
      "[37,   200] loss: 0.003\n",
      "[37,   400] loss: 0.001\n",
      "[37,   600] loss: 0.001\n",
      "[37,   800] loss: 0.001\n",
      "[37,  1000] loss: 0.002\n",
      "[37,  1200] loss: 0.002\n",
      "[37,  1400] loss: 0.001\n",
      "[37,  1600] loss: 0.002\n",
      "[37,  1800] loss: 0.004\n",
      "[38,   200] loss: 0.001\n",
      "[38,   400] loss: 0.001\n",
      "[38,   600] loss: 0.001\n",
      "[38,   800] loss: 0.001\n",
      "[38,  1000] loss: 0.000\n",
      "[38,  1200] loss: 0.003\n",
      "[38,  1400] loss: 0.002\n",
      "[38,  1600] loss: 0.002\n",
      "[38,  1800] loss: 0.002\n",
      "[39,   200] loss: 0.002\n",
      "[39,   400] loss: 0.002\n",
      "[39,   600] loss: 0.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39,   800] loss: 0.002\n",
      "[39,  1000] loss: 0.001\n",
      "[39,  1200] loss: 0.001\n",
      "[39,  1400] loss: 0.002\n",
      "[39,  1600] loss: 0.003\n",
      "[39,  1800] loss: 0.002\n",
      "[40,   200] loss: 0.001\n",
      "[40,   400] loss: 0.001\n",
      "[40,   600] loss: 0.001\n",
      "[40,   800] loss: 0.007\n",
      "[40,  1000] loss: 0.001\n",
      "[40,  1200] loss: 0.001\n",
      "[40,  1400] loss: 0.002\n",
      "[40,  1600] loss: 0.001\n",
      "[40,  1800] loss: 0.003\n",
      "[41,   200] loss: 0.001\n",
      "[41,   400] loss: 0.001\n",
      "[41,   600] loss: 0.002\n",
      "[41,   800] loss: 0.001\n",
      "[41,  1000] loss: 0.001\n",
      "[41,  1200] loss: 0.001\n",
      "[41,  1400] loss: 0.001\n",
      "[41,  1600] loss: 0.002\n",
      "[41,  1800] loss: 0.001\n",
      "[42,   200] loss: 0.001\n",
      "[42,   400] loss: 0.000\n",
      "[42,   600] loss: 0.001\n",
      "[42,   800] loss: 0.000\n",
      "[42,  1000] loss: 0.000\n",
      "[42,  1200] loss: 0.001\n",
      "[42,  1400] loss: 0.002\n",
      "[42,  1600] loss: 0.001\n",
      "[42,  1800] loss: 0.000\n",
      "[43,   200] loss: 0.004\n",
      "[43,   400] loss: 0.004\n",
      "[43,   600] loss: 0.002\n",
      "[43,   800] loss: 0.001\n",
      "[43,  1000] loss: 0.000\n",
      "[43,  1200] loss: 0.001\n",
      "[43,  1400] loss: 0.001\n",
      "[43,  1600] loss: 0.002\n",
      "[43,  1800] loss: 0.006\n",
      "[44,   200] loss: 0.001\n",
      "[44,   400] loss: 0.000\n",
      "[44,   600] loss: 0.001\n",
      "[44,   800] loss: 0.001\n",
      "[44,  1000] loss: 0.001\n",
      "[44,  1200] loss: 0.001\n",
      "[44,  1400] loss: 0.001\n",
      "[44,  1600] loss: 0.001\n",
      "[44,  1800] loss: 0.001\n",
      "[45,   200] loss: 0.001\n",
      "[45,   400] loss: 0.001\n",
      "[45,   600] loss: 0.001\n",
      "[45,   800] loss: 0.001\n",
      "[45,  1000] loss: 0.001\n",
      "[45,  1200] loss: 0.002\n",
      "[45,  1400] loss: 0.001\n",
      "[45,  1600] loss: 0.001\n",
      "[45,  1800] loss: 0.003\n",
      "[46,   200] loss: 0.000\n",
      "[46,   400] loss: 0.001\n",
      "[46,   600] loss: 0.004\n",
      "[46,   800] loss: 0.001\n",
      "[46,  1000] loss: 0.003\n",
      "[46,  1200] loss: 0.006\n",
      "[46,  1400] loss: 0.001\n",
      "[46,  1600] loss: 0.001\n",
      "[46,  1800] loss: 0.001\n",
      "[47,   200] loss: 0.002\n",
      "[47,   400] loss: 0.001\n",
      "[47,   600] loss: 0.001\n",
      "[47,   800] loss: 0.003\n",
      "[47,  1000] loss: 0.001\n",
      "[47,  1200] loss: 0.001\n",
      "[47,  1400] loss: 0.003\n",
      "[47,  1600] loss: 0.001\n",
      "[47,  1800] loss: 0.002\n",
      "[48,   200] loss: 0.001\n",
      "[48,   400] loss: 0.000\n",
      "[48,   600] loss: 0.001\n",
      "[48,   800] loss: 0.001\n",
      "[48,  1000] loss: 0.003\n",
      "[48,  1200] loss: 0.003\n",
      "[48,  1400] loss: 0.001\n",
      "[48,  1600] loss: 0.001\n",
      "[48,  1800] loss: 0.002\n",
      "[49,   200] loss: 0.003\n",
      "[49,   400] loss: 0.004\n",
      "[49,   600] loss: 0.001\n",
      "[49,   800] loss: 0.001\n",
      "[49,  1000] loss: 0.001\n",
      "[49,  1200] loss: 0.000\n",
      "[49,  1400] loss: 0.003\n",
      "[49,  1600] loss: 0.001\n",
      "[49,  1800] loss: 0.003\n",
      "[50,   200] loss: 0.001\n",
      "[50,   400] loss: 0.001\n",
      "[50,   600] loss: 0.001\n",
      "[50,   800] loss: 0.000\n",
      "[50,  1000] loss: 0.000\n",
      "[50,  1200] loss: 0.001\n",
      "[50,  1400] loss: 0.001\n",
      "[50,  1600] loss: 0.001\n",
      "[50,  1800] loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "   Train the model for 50 epochs and printing loss for each batch and epoch.\n",
    "\"\"\"\n",
    "epochs = 50                                        # number of epochs\n",
    "\n",
    "\n",
    "for epoch in range(epochs):                        # iterate over number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i,images in enumerate(train_data_loaded):  # iterate over the batches of training data\n",
    "        batch_images,batch_labels = images\n",
    "        batch_images = batch_images.to(device)     \n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(batch_images)              # computing forward pass \n",
    "        loss = criterion(outputs, batch_labels)    # Calculating loss\n",
    "        loss.backward()                            # Calculating the gradients\n",
    "        optimizer.step()                           # Updating the weights\n",
    "        \n",
    "        \"\"\"\n",
    "            Code below for printing of Losses for each batch and epoch was refered from the pytorch Documentation\n",
    "            https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "            \n",
    "        \"\"\"\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Saving the trained model\n",
    "\"\"\"\n",
    "torch.save(model.state_dict(), \"mnist.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
